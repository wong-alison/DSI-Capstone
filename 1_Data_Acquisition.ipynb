{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:21px\"><b>Data Acquisition</b></p>\n",
    "\n",
    "<b>To acquire the data I needed for this project I used the Steamwebapi along with the requests library.</b> <br><br> Steam has many APIs each returning information on a variety of different features. In order to acquire store front data of each game within the store I used the appdetails API which required an appid as a parameter.\n",
    "\n",
    "<br>\n",
    "To retrieve a list of appids from the store I used the ISteamApps API to get a list of game ids and name, using this to then obtain game information from StorefrontAPI. <br><br> The data was returned as a json type with 'applist' as a key and 'apps' as a key stored within this. I extracted this data and placed it into a dataframe, then stored as a csv to access later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option(\"max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.steampowered.com/ISteamApps/GetAppList/v2/'\n",
    "#This api only retrieves game names and id\n",
    "r = requests.get(url)\n",
    "json_ = r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json_['applist']['apps'][50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GameID = {'appid':[],\n",
    "               'name':[]}\n",
    "\n",
    "for item in json_['applist']['apps']:\n",
    "    try:\n",
    "        GameID['appid'].append(item['appid'])\n",
    "    except:\n",
    "        GameID['appid'].append('None')\n",
    "\n",
    "    try:\n",
    "         GameID['name'].append(item['name'])\n",
    "    except:\n",
    "        GameID.append('None')\n",
    "\n",
    "    \n",
    "GameIDs = pd.DataFrame(GameID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GameIDs.sort_values(by='appid', inplace=True)\n",
    "GameIDs.drop(GameIDs[(GameIDs.appid == 'None') | (GameIDs.name == 'None')].index, inplace=True)\n",
    "GameIDs.reset_index(drop=True, inplace=True)\n",
    "#GameIDs.to_csv('data/gameids.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are around 100k appids within the store so it took me a while to scrape the game data. In order to scrape this over a period of time without losing progress I used a few functions that will make an api call and then write the results to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_request(url, parameters=None):\n",
    "    \"\"\"Get request function to handle exceptions\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "    parameters : {'parameter': 'value'}\n",
    "           \n",
    "    Returns\n",
    "    -------\n",
    "    json_formatted data (dict like)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url=url, params=parameters)\n",
    " \n",
    "    \n",
    "    except requests.Timeout as errt:\n",
    "        print('Timeout Error:', errt)\n",
    "        #too many requests, pause and try again\n",
    "        print('Waiting 5 seconds')\n",
    "        time.sleep(5)\n",
    "        return get_request(url, parameters)\n",
    "    \n",
    "    except requests.ConnectionError as errc:\n",
    "        print('Connection Error:', errc)\n",
    "        #connection issue, pause and try again\n",
    "        print('Waiting 25 seconds')\n",
    "        time.sleep(25)\n",
    "        return get_request(url, parameters)\n",
    "        \n",
    "        \n",
    "    except requests.RequestException as erre:\n",
    "        print('General Error:', erre)\n",
    "        \n",
    "    \n",
    "    if response:\n",
    "        return response.json()\n",
    "    else:\n",
    "        # response is none usually means too many requests, pause and try again \n",
    "        print('No response, waiting 5 mins')\n",
    "        time.sleep(300)\n",
    "        print('Retrying')\n",
    "        return get_request(url, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(download_path, data_file, index_file):\n",
    "    \"\"\"\n",
    "    Function to create index file to save and retrieve progress\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    download_path : data folder\n",
    "    data_file : filename of saved data\n",
    "    index_file : filename for progress tracking\n",
    "           \n",
    "    Returns\n",
    "    -------\n",
    "    Current index\n",
    "    \"\"\"\n",
    "    index_path = os.path.join(download_path, index_file)\n",
    "    if os.path.isfile(index_path):\n",
    "        with open (index_path) as f:\n",
    "            index = int(f.read())\n",
    "    else:\n",
    "        index = 0\n",
    "        #If index is 0 create data file and write headers\n",
    "        data_path = os.path.join(download_path, data_file)\n",
    "        with open(data_path, 'w', newline='') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=columns)\n",
    "                writer.writeheader()\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_app_data(app_list, batchsize=100, pause=1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Retrieves data from steam api in batches and write to file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    app_list : list of apps to iterate through\n",
    "    n_apps : number of apps to process, by default will process all\n",
    "    batchsize : size of batches to write to file\n",
    "    pause : sleep timer to avoid overloading API\n",
    "           \n",
    "    \"\"\"    \n",
    "      \n",
    "    index = get_index(download_path, data_file, index_file)\n",
    "    print('Current index: {}'.format(index))    \n",
    "    end = len(app_list) + 1\n",
    "    \n",
    "        #set batches to process\n",
    "    batches = np.arange(index, end, batchsize)\n",
    "    \n",
    "    for i in tqdm(range(len(batches) -1 )):\n",
    "        \n",
    "        start = batches[i]\n",
    "        stop = batches [i+1]\n",
    "\n",
    "        app_data = []\n",
    "\n",
    "        # iterate through each row of app_list in batches\n",
    "        for index, row in app_list[start:stop].iterrows():\n",
    "\n",
    "            appid = row['appid']\n",
    "            name = row['name']\n",
    "\n",
    "            # retrive app data for a row and append to list\n",
    "\n",
    "            url = \"http://store.steampowered.com/api/appdetails/\"\n",
    "            parameters = {\"appids\": appid}\n",
    "\n",
    "            jr = get_request(url, parameters=parameters)\n",
    "            json_data = jr[str(appid)]\n",
    "\n",
    "            if json_data['success']: \n",
    "                data = json_data['data']\n",
    "            else:\n",
    "                data = {'name': name, 'steam_appid': appid}\n",
    "\n",
    "            app_data.append(data)\n",
    "            \n",
    "            time.sleep(pause) \n",
    "\n",
    "        # writing app data to file\n",
    "        data_path = os.path.join(download_path, data_file)\n",
    "\n",
    "        with open(data_path, 'a', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=columns, extrasaction='ignore')\n",
    "            print('Writing rows {}-{} '.format(start, stop-1))\n",
    "            writer.writerows(app_data)\n",
    "\n",
    "        # writing last index to file\n",
    "        index_path = os.path.join(download_path, index_file)\n",
    "        with open (index_path, 'w') as f:\n",
    "            f.write(str(stop))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file parameters\n",
    "download_path = 'data'\n",
    "index_file = 'steam_index.txt'\n",
    "data_file = 'steam_game_data.csv'\n",
    "app_list = pd.read_csv('data/gameids.csv')  \n",
    "\n",
    "columns = ['type', 'name', 'steam_appid', 'required_age', 'is_free', 'controller_support',\n",
    "    'dlc', 'detailed_description', 'about_the_game', 'short_description', 'fullgame',\n",
    "    'supported_languages', 'header_image', 'website', 'pc_requirements', 'mac_requirements',\n",
    "    'linux_requirements', 'legal_notice', 'drm_notice', 'ext_user_account_notice',\n",
    "    'developers', 'publishers', 'demos', 'price_overview', 'packages', 'package_groups',\n",
    "    'platforms', 'metacritic', 'reviews', 'categories', 'genres', 'screenshots',\n",
    "    'movies', 'recommendations', 'achievements', 'release_date', 'support_info',\n",
    "    'background', 'content_descriptors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current index: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [00:31<02:05, 31.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing rows 0-19 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [01:04<01:35, 31.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing rows 20-39 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [01:35<01:03, 31.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing rows 40-59 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [02:08<00:32, 32.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing rows 60-79 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:40<00:00, 32.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing rows 80-99 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#running scraping function for demonstration\n",
    "get_app_data(app_list, batchsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:21px\"><b>  Review Scraping</b></p> <br> Due to the vast number of games in the Steam Store and the tiem constraints of this project I decided to focus only on DLCs which was also a personal point of interest.\n",
    "<br><br>\n",
    "I extracted the appids with type DLC and then used these to loop through the reviews api. The number of reviews per game is very variable and some go up to 500k+ so I decided to only extract the first 100 reviews per DLC in English ordered by helpful-ness according to users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Review API takes the appid midway through the URL so I adjusted the get requests function\n",
    "def get_rev_request(url, appid):\n",
    "    \"\"\"Get request function to handle exceptions for reviews\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        json_formatted data (dict like)\n",
    "   \n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url.format(appid))\n",
    " \n",
    "    \n",
    "    except requests.Timeout as errt:\n",
    "        print('Timeout Error:', errt)\n",
    "        #too many requests, pause and try again\n",
    "        print('Waiting 5 seconds')\n",
    "        time.sleep(5)\n",
    "        return get_request(url, parameters)\n",
    "    \n",
    "    except requests.ConnectionError as errc:\n",
    "        print('Connection Error:', errc)\n",
    "        #connection issue, pause and try again\n",
    "        print('Waiting 25 seconds')\n",
    "        time.sleep(25)\n",
    "        return get_request(url, appid)\n",
    "        \n",
    "        \n",
    "    except requests.RequestException as erre:\n",
    "        print('General Error:', appid)\n",
    "        #print the appid here so I know which ones failed\n",
    "        \n",
    "    \n",
    "    if response:\n",
    "        return response.json()\n",
    "    else:\n",
    "        # response is none usually means too many requests, pause and try again \n",
    "        print('No response, waiting 5 mins')\n",
    "        time.sleep(300)\n",
    "        print('Retrying')\n",
    "        return get_request(url, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each request retreives a maximum of 100 reviews each time so write each file to page\n",
    "\n",
    "def get_review_data(appid_list, data_file, index_file, pause=1):\n",
    "    \n",
    "    \"\"\"Return app data generated from review api\n",
    "     \n",
    "     Parameters \n",
    "     ---------\n",
    "     appid_list : list of app ids in format appid_l['steam_appid']\n",
    "     data_file : filename of saved data\n",
    "     index_file : filename for progress tracking \n",
    "     pause :   sleep time\n",
    "    \"\"\"\n",
    "        \n",
    "    url = 'https://store.steampowered.com/appreviews/{}?json=1&language=english&num_per_page=100'\n",
    "    index_path = os.path.join(download_path, index_file)\n",
    "    \n",
    "    if os.path.isfile(index_path):\n",
    "        with open (index_path) as f:\n",
    "            index = int(f.read())\n",
    "    else:\n",
    "        index = 0\n",
    "\n",
    "    print('Current index: {}'.format(index))    \n",
    "\n",
    "    end = len(app_list) + 1\n",
    "    \n",
    "    batches = np.arange(index, end, 1)\n",
    "\n",
    "    for i in tqdm(range(len(batches) -1)):\n",
    "\n",
    "        start = batches[i]\n",
    "        stop = batches [i+1]\n",
    "\n",
    "        # iterate through each row of app_list \n",
    "        for index, row in app_list[start:stop].iterrows():\n",
    "\n",
    "            appid = row['steam_appid']\n",
    "\n",
    "            # retrieve app data for a row and send to DF\n",
    "\n",
    "            data = get_rev_request(url, appid)\n",
    "\n",
    "            if (data['success'] == 1):\n",
    "                if data.get('reviews'): #this checks if there are reviews\n",
    "                    data = data['reviews']\n",
    "                else:     \n",
    "                    data = {'review': ['No reviews']}\n",
    "            else:\n",
    "                data = {'review': ['Failed to scrape']}\n",
    "\n",
    "\n",
    "            data = pd.DataFrame.from_dict(data)\n",
    "\n",
    "            data.insert(0, 'dlc', 1)\n",
    "            data.insert(0,'app_id', appid)\n",
    "\n",
    "            time.sleep(pause) \n",
    "\n",
    "            data_path = os.path.join(download_path, data_file)\n",
    "\n",
    "            #write df to csv, doesn't write headers if they exist\n",
    "\n",
    "            with open(data_path, 'a', encoding='utf-8') as f:\n",
    "                    data.to_csv(f, header=f.tell()==0)\n",
    "\n",
    "            # writing last index to file\n",
    "            with open (index_path, 'w') as f:\n",
    "                f.write(str(stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in Game DF to get a list of appids, use this to retrieve review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/steam_game_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(keep='first', inplace=True) \n",
    "dlc_df = df[df['type']== 'dlc']\n",
    "dlc_l = dlc_df[['type', 'steam_appid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_index = 'steam_dlc_index.txt'\n",
    "review_file = 'steam_dlc_review.csv'\n",
    "#Call function to get dlc review data\n",
    "get_review_data(appid_list=dlc_l, data_file=review_file, index_file=review_index, pause=3):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
