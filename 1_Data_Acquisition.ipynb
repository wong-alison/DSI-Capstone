{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:21px\"><b>Data Acquisition</b></p>\n",
    "\n",
    "<b>To acquire the data I needed for this project I used the Steamwebapi along with the requests library.</b> <br><br> Steam has many APIs each returning information on a variety of different features. In order to acquire store front data of each game within the Steam Store I used the appdetails API which required an appid as a parameter.\n",
    "\n",
    "<br>\n",
    "To retrieve a list of appids from the store I used the ISteamApps API to get a list of game ids and name, using this to then obtain game information from StorefrontAPI. <br><br> The data was returned as a json type with 'applist' as a key and 'apps' as a key stored within this. I extracted this data and placed it into a dataframe, then stored as a csv to access later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "import csv\n",
    "import datetime as dt\n",
    "import json\n",
    "import os\n",
    "import statistics\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# customisations - ensure tables show all columns\n",
    "pd.set_option(\"max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.steampowered.com/ISteamApps/GetAppList/v2/'\n",
    "#This api only retrieves game names and id\n",
    "r = requests.get(url)\n",
    "json_ = r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'appid': 1506220, 'name': 'Fantasy Grounds - Sherwood: The Legend of Robin Hood'}\n"
     ]
    }
   ],
   "source": [
    "print(json_['applist']['apps'][50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GameID = {'appid':[],\n",
    "               'name':[]}\n",
    "\n",
    "for item in json_['applist']['apps']:\n",
    "    try:\n",
    "        GameID['appid'].append(item['appid'])\n",
    "    except:\n",
    "        GameID['appid'].append('None')\n",
    "\n",
    "    try:\n",
    "         GameID['name'].append(item['name'])\n",
    "    except:\n",
    "        GameID.append('None')\n",
    "\n",
    "    \n",
    "GameIDs = pd.DataFrame(GameID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GameIDs.sort_values(by='appid', inplace=True)\n",
    "GameIDs.drop(GameIDs[(GameIDs.appid == 'None') | (GameIDs.name == 'None')].index, inplace=True)\n",
    "GameIDs.reset_index(drop=True, inplace=True)\n",
    "#GameIDs.to_csv('data/gameids.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are around 100k appids within the store so it took me a while to scrape the game data. In order to scrape this over a period of time without losing progress I used a few functions that will make an api call and then write the results to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining get request functon that will handle exceptions\n",
    "\n",
    "def get_request(url, parameters=None):\n",
    "    \"\"\"Return json-formatted response of a get request using parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "    parameters : {'parameter': 'value'}\n",
    "           \n",
    "    Returns\n",
    "    -------\n",
    "    json_data\n",
    "        json-formatted response (dict-like)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url=url, params=parameters)\n",
    " \n",
    "    \n",
    "    except requests.Timeout as errt:\n",
    "        print('Timeout Error:', errt)\n",
    "        #too many requests, pause and try again\n",
    "        print('Waiting 5 seconds')\n",
    "        time.sleep(5)\n",
    "        return get_request(url, parameters)\n",
    "    \n",
    "    except requests.ConnectionError as errc:\n",
    "        print('Connection Error:', errc)\n",
    "        #connection issue, pause and try again\n",
    "        print('Waiting 25 seconds')\n",
    "        time.sleep(25)\n",
    "        return get_request(url, parameters)\n",
    "              \n",
    "\n",
    "    except KeyboardInterrupt as errk:\n",
    "        print('Program stopped:', errk)\n",
    "        \n",
    "        \n",
    "    except requests.RequestException as erre:\n",
    "        print('General Error:', erre)\n",
    "        \n",
    "    \n",
    "    if response:\n",
    "        return response.json()\n",
    "    else:\n",
    "        # response is none usually means too many requests, pause and try again \n",
    "        print('No response, waiting 5 mins')\n",
    "        time.sleep(300)\n",
    "        print('Retrying')\n",
    "        return get_request(url, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_app_data(start, stop, parser, pause):\n",
    "    \"\"\"Return app data from Steam Store API: json formatted\n",
    "    \"\"\"\n",
    "    app_data = []\n",
    "    \n",
    "    # iterate through each row of app_list, confined by start and stop\n",
    "    for index, row in app_list[start:stop].iterrows():\n",
    "        print('Current index: {}'.format(index), end='\\r')\n",
    "        \n",
    "        appid = row['appid']\n",
    "        name = row['name']\n",
    "\n",
    "        # retrive app data for a row and append to list\n",
    "        \n",
    "        url = \"http://store.steampowered.com/api/appdetails/\"\n",
    "        parameters = {\"appids\": appid}\n",
    "\n",
    "        json_data = get_request(url, parameters=parameters)\n",
    "        json_app_data = json_data[str(appid)]\n",
    "\n",
    "        if json_app_data['success']: \n",
    "            data = json_app_data['data']\n",
    "        else:\n",
    "            data = {'name': name, 'steam_appid': appid}\n",
    "        \n",
    "        \n",
    "        app_data.append(data)\n",
    "\n",
    "        time.sleep(pause) # prevent overloading api with requests\n",
    "    \n",
    "    return app_data\n",
    "\n",
    "\n",
    "def process_batches(parser, app_list, download_path, data_filename, index_filename,\n",
    "                    columns, begin=0, end=-1, batchsize=100, pause=1):\n",
    "    \"\"\"Process app data in batches, writing directly to file.\n",
    "    \n",
    "    parser : custom function to format request\n",
    "    app_list : dataframe of appid and name\n",
    "    download_path : path to store data\n",
    "    data_filename : filename to save app data\n",
    "    index_filename : filename to store highest index written\n",
    "    columns : column names for file\n",
    "    \n",
    "    Keyword arguments:\n",
    "    \n",
    "    begin : starting index (get from index_filename, default 0)\n",
    "    end : index to finish (defaults to end of app_list)\n",
    "    batchsize : number of apps to write in each batch (default 100)\n",
    "    pause : time to wait after each api request (defualt 1)\n",
    "    \n",
    "    returns: none\n",
    "    \"\"\"\n",
    "    print('Starting at index {}:\\n'.format(begin))\n",
    "    \n",
    "    # by default, process all apps in app_list\n",
    "    if end == -1:\n",
    "        end = len(app_list) + 1\n",
    "    \n",
    "    # generate array of batch begin and end points\n",
    "    batches = np.arange(begin, end, batchsize)\n",
    "    batches = np.append(batches, end)\n",
    "    \n",
    "    apps_written = 0\n",
    "    batch_times = []\n",
    "    \n",
    "    for i in range(len(batches) - 1):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        start = batches[i]\n",
    "        stop = batches[i+1]\n",
    "        \n",
    "        app_data = get_app_data(start, stop, parser, pause)\n",
    "        \n",
    "        rel_path = os.path.join(download_path, data_filename)\n",
    "        \n",
    "        # writing app data to file\n",
    "        with open(rel_path, 'a', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=columns, extrasaction='ignore')\n",
    "            \n",
    "            for j in range(3,0,-1):\n",
    "                print(\"\\rAbout to write data, don't stop script! ({})\".format(j), end='')\n",
    "                time.sleep(0.5)\n",
    "            \n",
    "            writer.writerows(app_data)\n",
    "            print('\\rExported lines {}-{} to {}.'.format(start, stop-1, data_filename), end=' ')\n",
    "            \n",
    "        apps_written += len(app_data)\n",
    "        \n",
    "        idx_path = os.path.join(download_path, index_filename)\n",
    "        \n",
    "        # writing last index to file\n",
    "        with open(idx_path, 'w') as f:\n",
    "            index = stop\n",
    "            print(index, file=f)\n",
    "            \n",
    "        # logging time taken\n",
    "        end_time = time.time()\n",
    "        time_taken = end_time - start_time\n",
    "        \n",
    "        batch_times.append(time_taken)\n",
    "        mean_time = statistics.mean(batch_times)\n",
    "        \n",
    "        est_remaining = (len(batches) - i - 2) * mean_time\n",
    "        \n",
    "        remaining_td = dt.timedelta(seconds=round(est_remaining))\n",
    "        time_td = dt.timedelta(seconds=round(time_taken))\n",
    "        mean_td = dt.timedelta(seconds=round(mean_time))\n",
    "        \n",
    "        print('Batch {} time: {} (avg: {}, remaining: {})'.format(i, time_td, mean_td, remaining_td))\n",
    "            \n",
    "    print('\\nProcessing batches complete. {} apps written'.format(apps_written))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(download_path, index_filename):\n",
    "    \"\"\"Retrieve index from file, returning 0 if file not found.\"\"\"\n",
    "    try:\n",
    "        rel_path = os.path.join(download_path, index_filename)\n",
    "\n",
    "        with open(rel_path, 'r') as f:\n",
    "            index = int(f.readline())\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        index = 0\n",
    "        \n",
    "    return index\n",
    "\n",
    "\n",
    "def prepare_data_file(download_path, filename, index, columns):\n",
    "    \"\"\"Create file and write headers if index is 0.\"\"\"\n",
    "    if index == 0:\n",
    "        rel_path = os.path.join(download_path, filename)\n",
    "\n",
    "        with open(rel_path, 'w', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=columns)\n",
    "            writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_steam_request(appid, name):\n",
    "    \"\"\"Unique parser to handle data from Steam Store API.\n",
    "    \n",
    "    Returns : json formatted data (dict-like)\n",
    "    \"\"\"\n",
    "    url = \"http://store.steampowered.com/api/appdetails/\"\n",
    "    parameters = {\"appids\": appid}\n",
    "    \n",
    "    json_data = get_request(url, parameters=parameters)\n",
    "    json_app_data = json_data[str(appid)]\n",
    "    \n",
    "    if json_app_data['success']:\n",
    "        data = json_app_data['data']\n",
    "    else:\n",
    "        data = {'name': name, 'steam_appid': appid}\n",
    "        \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_list = pd.read_csv('data/gameids.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/alison/Desktop/GA/DSI15-lessons/project/project-capstone/Capstone Steam'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file parameters\n",
    "download_path = 'data/download'\n",
    "steam_app_data = 'steam_game_data2.csv'\n",
    "steam_index = 'steam_index2.txt'\n",
    "\n",
    "steam_columns = [\n",
    "    'type', 'name', 'steam_appid', 'required_age', 'is_free', 'controller_support',\n",
    "    'dlc', 'detailed_description', 'about_the_game', 'short_description', 'fullgame',\n",
    "    'supported_languages', 'header_image', 'website', 'pc_requirements', 'mac_requirements',\n",
    "    'linux_requirements', 'legal_notice', 'drm_notice', 'ext_user_account_notice',\n",
    "    'developers', 'publishers', 'demos', 'price_overview', 'packages', 'package_groups',\n",
    "    'platforms', 'metacritic', 'reviews', 'categories', 'genres', 'screenshots',\n",
    "    'movies', 'recommendations', 'achievements', 'release_date', 'support_info',\n",
    "    'background', 'content_descriptors'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at index 8260:\n",
      "\n",
      "Exported lines 8260-8359 to steam_game_data2.csv. Batch 0 time: 0:03:18 (avg: 0:03:18, remaining: 2 days, 7:26:44)\n",
      "Exported lines 8360-8459 to steam_game_data2.csv. Batch 1 time: 0:03:27 (avg: 0:03:23, remaining: 2 days, 8:35:34)\n",
      "Exported lines 8460-8559 to steam_game_data2.csv. Batch 2 time: 0:03:34 (avg: 0:03:26, remaining: 2 days, 9:36:20)\n",
      "Exported lines 8560-8659 to steam_game_data2.csv. Batch 3 time: 0:03:19 (avg: 0:03:25, remaining: 2 days, 9:04:01)\n",
      "Exported lines 8660-8759 to steam_game_data2.csv. Batch 4 time: 0:03:24 (avg: 0:03:25, remaining: 2 days, 8:58:39)\n",
      "Current index: 8796\r"
     ]
    }
   ],
   "source": [
    "# Retrieve last index downloaded from file\n",
    "index = get_index(download_path, steam_index)\n",
    "\n",
    "# Wipe or create data file and write headers if index is 0\n",
    "prepare_data_file(download_path, steam_app_data, index, steam_columns)\n",
    "\n",
    "# Set end and chunksize for demonstration - remove to run through entire app list\n",
    "process_batches(\n",
    "    parser=parse_steam_request,\n",
    "    app_list=app_list,\n",
    "    download_path=download_path,\n",
    "    data_filename=steam_app_data,\n",
    "    index_filename=steam_index,\n",
    "    columns=steam_columns,\n",
    "    begin=index#,\n",
    "    #end=600,\n",
    "    #batchsize=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check number of apps writen to file match the original app list\n",
    "app_list.info() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:21px\"><b>Review Scraping</b></p> <br> Due to the vast number of games in the Steam Store and the limited amount of time I had to carry out this project these I decided to focus only on DLCs which was also a personal point of interest.\n",
    "<br><br>\n",
    "I extracted the appids with type DLC and then used these to loop through the reviews api. The number of reviews per game is very variable and some go up to 500k+ so I decided to only extract 100 reviews per DLC in English ranked by helpful-ness according to users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Review API takes the appid midway through the URL so I adjusted the get requests function\n",
    "def get_rev_request(url, appid):\n",
    "    \"\"\"Return json-formatted response of a get request using parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "    parameters : {'parameter': 'value'}\n",
    "           \n",
    "    Returns\n",
    "    -------\n",
    "    json_data\n",
    "        json-formatted response (dict-like)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url.format(appid))\n",
    " \n",
    "    \n",
    "    except requests.Timeout as errt:\n",
    "        print('Timeout Error:', errt)\n",
    "        #too many requests, pause and try again\n",
    "        print('Waiting 5 seconds')\n",
    "        time.sleep(5)\n",
    "        return get_request(url, parameters)\n",
    "    \n",
    "    except requests.ConnectionError as errc:\n",
    "        print('Connection Error:', errc)\n",
    "        #connection issue, pause and try again\n",
    "        print('Waiting 25 seconds')\n",
    "        time.sleep(25)\n",
    "        return get_request(url, appid)\n",
    "              \n",
    "\n",
    "    except KeyboardInterrupt as errk:\n",
    "        print('Program stopped:', errk)\n",
    "        \n",
    "        \n",
    "    except requests.RequestException as erre:\n",
    "        print('General Error:', appid)\n",
    "        #print the appid here so I know which ones failed\n",
    "        \n",
    "    \n",
    "    if response:\n",
    "        return response.json()\n",
    "    else:\n",
    "        # response is none usually means too many requests, pause and try again \n",
    "        print('No response, waiting 5 mins')\n",
    "        time.sleep(300)\n",
    "        print('Retrying')\n",
    "        return get_request(url, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_data(appid_l, pause=1):\n",
    "    \"\"\"Return app data generated from review api\n",
    "     \n",
    "     Parametres \n",
    "     ---------\n",
    "     appid_l \n",
    "         list of app ids in format appid_l['steam_appid']\n",
    "     pause \n",
    "         sleep time\n",
    "    \"\"\"\n",
    "    #to check time it took to run\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    url = 'https://store.steampowered.com/appreviews/{}?json=1&language=english&num_per_page=100'\n",
    "    \n",
    "    # iterate through each row of app_list \n",
    "    for appid in appid_l['steam_appid']:\n",
    "        \n",
    "        # retrieve app data for a row and send to DF\n",
    "       \n",
    "        data = get_request(url, appid)\n",
    "        \n",
    "        if (data['success'] == 1):\n",
    "            if data.get('reviews'): #this checks if there are reviews\n",
    "                data = data['reviews']\n",
    "            else:     \n",
    "                data = {'review': ['No reviews']}\n",
    "        else:\n",
    "            data = {'review': ['Failed to scrape']}\n",
    "\n",
    "          \n",
    "        data = pd.DataFrame.from_dict(data)\n",
    "       \n",
    "        data.insert(0, 'dlc', 1)\n",
    "        data.insert(0,'app_id', appid)\n",
    "\n",
    "        time.sleep(pause) # prevent overloading api with requests\n",
    "        \n",
    "        rel_path = os.path.join(download_path, data_filename)\n",
    "        \n",
    "        #write df to csv, doesn't write headers if they exist\n",
    "    \n",
    "        with open(rel_path, 'a', encoding='utf-8') as f:\n",
    "                data.to_csv(f, header=f.tell()==0)\n",
    "                \n",
    "    end = time.time() \n",
    "    print('\\nThe function took {:.2f} s to compute.'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in Game DF to get a list of appids, theres a lot so will partition these to process in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamedf = pd.read_csv('data/download/steam_game_data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamedf.drop_duplicates(keep='first', inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlc_df = gamedf[gamedf['type']== 'dlc']\n",
    "dlc_l = dlc_df[['type', 'steam_appid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlc_l.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define n partitions to run\n",
    "27325/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = 27\n",
    "part_dfs = np.array_split(dlc_l, partitions)\n",
    "download_path = 'data/download'\n",
    "data_filename = 'steam_dlc_reviews2.csv'\n",
    "#Call function to get dlc review data\n",
    "#get_app_data(part_dfs[0], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in to inspect dlc review data\n",
    "\n",
    "#dlc0_df = pd.read_csv('data/download/steam_dlc_reviews2.csv',  \n",
    "                      usecols = ['app_id', 'dlc', 'recommendationid', 'author'], encoding= 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Upon inspection I noticed from appids were missin so I created a function to check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkIfValuesExists(df_main, df_scrape):\n",
    "    ''' Check if given elements exists in a dataframe or not.\n",
    "        Returns a dataframe with the missing ids'''\n",
    "    df_check = df_main.assign(appid=df_main.steam_appid.isin(df_scrape.app_id).astype(int))\n",
    "    missing_id = df_check[df_check['appid']==0]\n",
    "    \n",
    "    return missing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remain_l = checkIfValuesExists(dlc_l, dlc0_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_path = 'data/download/review'\n",
    "data_filename = 'steam_dlc_reviews2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rerun scrape anything missed\n",
    "get_app_data(part_dfs[3], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlc_df = pd.read_csv('data/download/steam_dlc_reviews2.csv',  \n",
    "                      usecols = ['app_id', 'dlc', 'recommendationid', 'author'], encoding= 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remain_l2 = checkIfValuesExists(dlc_l, dlc_df)\n",
    "len(remain_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
